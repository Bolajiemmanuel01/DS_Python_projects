{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a90e43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import neccesary Libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from getpass import getpass\n",
    "import mysql.connector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59c9ab6",
   "metadata": {},
   "source": [
    "## Extract State Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce59e802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here i want to extract the state paths so i can be easy to loop over and retrieve the json files\n",
    "\n",
    "# Get the data path\n",
    "directory = 'C:\\\\Users\\\\a\\\\OneDrive\\\\Desktop\\\\Github_projects\\\\pulse\\\\data\\\\'\n",
    "\n",
    "\n",
    "# Function to extract every path subfolders that contains state\n",
    "def state_path(directory):\n",
    "    state_paths = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        if 'state' in dirs:\n",
    "            state_dir = os.path.join(root, 'state')\n",
    "            state_dir = state_dir.replace('\\\\', '/')\n",
    "            state_paths.append(state_dir)\n",
    "    return state_paths\n",
    "\n",
    "# Call the function to extract the state paths\n",
    "state_paths = state_path(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "867d7bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/a/OneDrive/Desktop/Github_projects/pulse/data/aggregated/transaction/country/india/state\n",
      "C:/Users/a/OneDrive/Desktop/Github_projects/pulse/data/aggregated/user/country/india/state\n",
      "C:/Users/a/OneDrive/Desktop/Github_projects/pulse/data/map/transaction/hover/country/india/state\n",
      "C:/Users/a/OneDrive/Desktop/Github_projects/pulse/data/map/user/hover/country/india/state\n",
      "C:/Users/a/OneDrive/Desktop/Github_projects/pulse/data/top/transaction/country/india/state\n",
      "C:/Users/a/OneDrive/Desktop/Github_projects/pulse/data/top/user/country/india/state\n"
     ]
    }
   ],
   "source": [
    "# Let's check the paths\n",
    "for path in state_paths:\n",
    "    print(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb492c4",
   "metadata": {},
   "source": [
    "Here we can see that we have 6 paths that contains state subfolders, but something common about it is that they're categorized into three, Agggregate, Map, and Top, they all contain two types of data User data and Transaction data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400e0998",
   "metadata": {},
   "source": [
    "## Extract Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32201aaf",
   "metadata": {},
   "source": [
    "Since we have all paths the next thing is to extract the data set, using the path. But the state subfolders do not contain the data, rather it contains each state in India subfolders, then each state also contains year subfolders which contains 4 sets of Json file which represents the each quater which is what we want to extract. The best thing to do is to create a function for each path which will iterate over all this subfolder then extrat the Json files orderly, and we will be able to keep track of Each State, Year, and Quater of the files we're extracting. The function is going to return a dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd26de2",
   "metadata": {},
   "source": [
    "**Aggregate Transaction Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f0cc695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the function to extract Aggregate Transaction data, this will accept the path to the aggregate transaction \n",
    "\n",
    "def extract_agg_transact_data(path):\n",
    "    #Create a dictionary which contains an empty list to store each items that will be retrieved\n",
    "    agg_data = {\n",
    "        'State':[], 'Year':[], 'Quarter':[],\n",
    "        'Payment_Type':[], 'Total_Transaction':[], 'Transaction_Value':[]\n",
    "    }\n",
    "    # loop over each state in the states subfolder\n",
    "    for state in os.listdir(path):\n",
    "        # Add each state to the path\n",
    "        state_path = f\"{path}/{state}\"\n",
    "        # loop over each year in the state path derived above\n",
    "        for year in os.listdir(state_path):\n",
    "            # add each year to the path \n",
    "            year_path = f\"{state_path}/{year}\"\n",
    "            # loop over the json files\n",
    "            for json_file in os.listdir(year_path):\n",
    "                # add each file to the path\n",
    "                json_path = f\"{year_path}/{json_file}\"\n",
    "                # Retreive what quater we're currently extracting, by separating the .json handle\n",
    "                json_f = json_file.split('.json')[0]\n",
    "                # convert to a dataframe for easier extraction\n",
    "                df = pd.read_json(json_path)\n",
    "                # Chose exactly where we want to extract the data on the dataframe\n",
    "                data_to_extract = df['data']['transactionData']\n",
    "                # Loop through the data to extract what we want\n",
    "                for data in data_to_extract:\n",
    "                    # Extract the payment type and append it to the appropriate list in the dictionary\n",
    "                    agg_data['Payment_Type'].append(data['name'])\n",
    "                    # Extract the Total Transaction and append it to the appropriate list in the dictionary\n",
    "                    agg_data['Total_Transaction'].append(data['paymentInstruments'][0]['count'])\n",
    "                    # Extract the Total Value and append it to the appropriate list in the dictionary\n",
    "                    agg_data['Transaction_Value'].append(data['paymentInstruments'][0]['amount'])\n",
    "                    # Here we retrieve the current state we're extracting from remove uneccesary symbols, clean and Capitalize then append it to the appropriate list in the dictionary\n",
    "                    agg_data['State'].append(state.title().replace('-', ' ').replace('&', 'and'))\n",
    "                    # Here we retrieve the year and append it to the appropriate list in the dictionary\n",
    "                    agg_data['Year'].append(year)\n",
    "                    # Reterieve the Quater we're curently extracting from and append it to the appropriate list in the dictionary\n",
    "                    agg_data['Quarter'].append(json_f)\n",
    "    # We convert the Dictionary into a Dataframe\n",
    "    df_tran = pd.DataFrame(agg_data)\n",
    "    # Return the Dataframe\n",
    "    return df_tran"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e0143c",
   "metadata": {},
   "source": [
    "**Aggregate User Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9192b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the function to extract Aggregate User data, this will accept the path to the aggregate user\n",
    "\n",
    "def extract_agg_user_data(path):\n",
    "    #Create a dictionary which contains an empty list to store each items that will be retrieved\n",
    "    agg_user_data = {\n",
    "        'State':[], 'Year':[], 'Quarter':[],\n",
    "        'Registered_user':[], 'App_Open':[], 'Phone_Brand':[], 'Registered_user_by_Phone_Brand':[]\n",
    "    }\n",
    "    # loop over each state in the states subfolder\n",
    "    for state in os.listdir(path):\n",
    "        # Add each state to the path\n",
    "        state_path = f\"{path}/{state}\"\n",
    "        # loop over each year in the state path derived above\n",
    "        for year in os.listdir(state_path):\n",
    "            # add each year to the path\n",
    "            year_path = f\"{state_path}/{year}\"\n",
    "            # loop over the json files\n",
    "            for json_file in os.listdir(year_path):\n",
    "                # add each file to the path\n",
    "                json_path = f\"{year_path}/{json_file}\"\n",
    "                # Retreive what quater we're currently extracting, by separating the .json handle\n",
    "                json_f = json_file.split('.json')[0]\n",
    "                # convert to a dataframe for easier extraction\n",
    "                df = pd.read_json(json_path)\n",
    "                # Chose exactly where we want to extract the data on the dataframe\n",
    "                data_agg_to_extract = df['data']['aggregated']\n",
    "                data_to_extract = df['data']['usersByDevice']\n",
    "                # check if data_to_extract isn't empty, so as to prevent looping through an empty list\n",
    "                if data_to_extract is not None:\n",
    "                    # if it's not empty, then loop over it\n",
    "                    for data in data_to_extract:\n",
    "                        # Check if it's not empty\n",
    "                        if data is not None:\n",
    "                            # Extract the registered users and append it to the appropriate list in the dictionary\n",
    "                            agg_user_data['Registered_user'].append(data_agg_to_extract['registeredUsers'])\n",
    "                            # Extract the No of App oepn and append it to the appropriate list in the dictionary\n",
    "                            agg_user_data['App_Open'].append(data_agg_to_extract['appOpens'])\n",
    "                            # Extract the Phone Brand and append it to the appropriate list in the dictionary\n",
    "                            agg_user_data['Phone_Brand'].append(data['brand'])\n",
    "                            # Extract the Registered users by phone brand and append it to the appropriate list in the dictionary\n",
    "                            agg_user_data['Registered_user_by_Phone_Brand'].append(data['count'])\n",
    "                            # Here we retrieve the current state we're extracting from remove uneccesary symbols, clean and Capitalize then append it to the appropriate list in the dictionary\n",
    "                            agg_user_data['State'].append(state.title().replace('-', ' ').replace('&', 'and'))\n",
    "                            # Here we retrieve the year and append it to the appropriate list in the dictionary\n",
    "                            agg_user_data['Year'].append(year)\n",
    "                            # Reterieve the Quater we're curently extracting from and append it to the appropriate list in the dictionary\n",
    "                            agg_user_data['Quarter'].append(json_f)\n",
    "    # We convert the Dictionary into a Dataframe\n",
    "    df_user = pd.DataFrame(agg_user_data)\n",
    "    # Return the Dataframe\n",
    "    return df_user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e8e0e7",
   "metadata": {},
   "source": [
    "**Map Transaction Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ec2cb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the function to extract Map Transaction data, this will accept the path to the Map Transaction\n",
    "\n",
    "def extract_map_transact_data(path):\n",
    "    #Create a dictionary which contains an empty list to store each items that will be retrieved\n",
    "    map_trans_data = {\n",
    "        'State':[], 'Year':[], 'Quarter':[],\n",
    "        'District':[], 'Total_Transaction':[], 'Transaction_Value':[]\n",
    "    }\n",
    "    # loop over each state in the states subfolder\n",
    "    for state in os.listdir(path):\n",
    "        # Add each state to the path\n",
    "        state_path = f\"{path}/{state}\"\n",
    "        # loop over each year in the state path derived above\n",
    "        for year in os.listdir(state_path):\n",
    "            # add each year to the path\n",
    "            year_path = f\"{state_path}/{year}\"\n",
    "            # loop over the json files\n",
    "            for json_file in os.listdir(year_path):\n",
    "                # add each file to the path\n",
    "                json_path = f\"{year_path}/{json_file}\"\n",
    "                # Retreive what quater we're currently extracting, by separating the .json handle\n",
    "                json_f = json_file.split('.json')[0]\n",
    "                # convert to a dataframe for easier extraction\n",
    "                df = pd.read_json(json_path)\n",
    "                # Loop through the data to extract what we want\n",
    "                for data in df['data']['hoverDataList']:\n",
    "                    # Extract the District and append it to the appropriate list in the dictionary\n",
    "                    map_trans_data['District'].append(data['name'].title().replace(' And', ' and').replace('andaman', 'Andaman'))\n",
    "                    # Extract the Total Transaction and append it to the appropriate list in the dictionary\n",
    "                    map_trans_data['Total_Transaction'].append(data['metric'][0]['count'])\n",
    "                    # Extract the Transaction Value and append it to the appropriate list in the dictionary\n",
    "                    map_trans_data['Transaction_Value'].append(data['metric'][0]['amount'])\n",
    "                    # Here we retrieve the current state we're extracting from remove uneccesary symbols, clean and Capitalize then append it to the appropriate list in the dictionary\n",
    "                    map_trans_data['State'].append(state.title().replace('-', ' ').replace('&', 'and'))\n",
    "                    # Here we retrieve the year and append it to the appropriate list in the dictionary\n",
    "                    map_trans_data['Year'].append(year)\n",
    "                    # Reterieve the Quater we're curently extracting from and append it to the appropriate list in the dictionary\n",
    "                    map_trans_data['Quarter'].append(json_f)\n",
    "    # We convert the Dictionary into a Dataframe\n",
    "    df_map_trans = pd.DataFrame(map_trans_data)\n",
    "    # Return the Dataframe\n",
    "    return df_map_trans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8a7bea",
   "metadata": {},
   "source": [
    "**Map User Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "843e9fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the function to extract Map User data, this will accept the path to the Map User\n",
    "\n",
    "def extract_map_user_data(path):\n",
    "    #Create a dictionary which contains an empty list to store each items that will be retrieved\n",
    "    map_user_data = {\n",
    "        'State':[], 'Year':[], 'Quarter':[],\n",
    "        'District':[], 'Total_Registered_User':[], 'Total_App_open':[]\n",
    "    }\n",
    "    # loop over each state in the states subfolder\n",
    "    for state in os.listdir(path):\n",
    "        # Add each state to the path\n",
    "        state_path = f\"{path}/{state}\"\n",
    "        # loop over each year in the state path derived above\n",
    "        for year in os.listdir(state_path):\n",
    "            # add each year to the path\n",
    "            year_path = f\"{state_path}/{year}\"\n",
    "            # loop over the json files\n",
    "            for json_file in os.listdir(year_path):\n",
    "                # add each file to the path\n",
    "                json_path = f\"{year_path}/{json_file}\"\n",
    "                # Retreive what quater we're currently extracting, by separating the .json handle\n",
    "                json_f = json_file.split('.json')[0]\n",
    "                # convert to a dataframe for easier extraction\n",
    "                df = pd.read_json(json_path)\n",
    "                # Loop through the data to extract what we want\n",
    "                for district, userdata in df['data']['hoverData'].items():\n",
    "                    # Extract the District and append it to the appropriate list in the dictionary\n",
    "                    map_user_data['District'].append(district.title().replace(' And', ' and').replace('andaman', 'Andaman'))\n",
    "                    # Extract the Total registered user and append it to the appropriate list in the dictionary\n",
    "                    map_user_data['Total_Registered_User'].append(userdata['registeredUsers'])\n",
    "                    # Extract the Total App opened and append it to the appropriate list in the dictionary\n",
    "                    map_user_data['Total_App_open'].append(userdata['registeredUsers'])\n",
    "                    # Here we retrieve the current state we're extracting from remove uneccesary symbols, clean and Capitalize then append it to the appropriate list in the dictionary\n",
    "                    map_user_data['State'].append(state.title().replace('-', ' ').replace('&', 'and'))\n",
    "                    # Here we retrieve the year and append it to the appropriate list in the dictionary\n",
    "                    map_user_data['Year'].append(year)\n",
    "                    # Reterieve the Quater we're curently extracting from and append it to the appropriate list in the dictionary\n",
    "                    map_user_data['Quarter'].append(json_f)\n",
    "    # We convert the Dictionary into a Dataframe\n",
    "    df_map_user = pd.DataFrame(map_user_data)            \n",
    "    # Return the Dataframe\n",
    "    return df_map_user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70075851",
   "metadata": {},
   "source": [
    "**Top Transaction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c238d9",
   "metadata": {},
   "source": [
    "For Top data, we would need to split our extraction functions, one for district and the other for pincodes because in the dataset they do not have equal amount of data i.e they do not have the same shape(Row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a75e68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the function to extract Top Transaction District data, this will accept the path to the Top Transaction\n",
    "\n",
    "def extract_top_transact_district_data(path):\n",
    "    #Create a dictionary which contains an empty list to store each items that will be retrieved\n",
    "    top_transact_district = {\n",
    "        'State':[], 'Year':[], 'Quarter':[],\n",
    "        'District':[], 'Total_Transaction': [], 'Transaction_Value': []\n",
    "    }\n",
    "    # loop over each state in the states subfolder\n",
    "    for state in os.listdir(path):\n",
    "        # Add each state to the path\n",
    "        state_path = f\"{path}/{state}\"\n",
    "        # loop over each year in the state path derived above\n",
    "        for year in os.listdir(state_path):\n",
    "            # add each year to the path\n",
    "            year_path = f\"{state_path}/{year}\"\n",
    "            # loop over the json files\n",
    "            for json_file in os.listdir(year_path):\n",
    "                # add each file to the path\n",
    "                json_path = f\"{year_path}/{json_file}\"\n",
    "                # Retreive what quater we're currently extracting, by separating the .json handle\n",
    "                json_f = json_file.split('.json')[0]\n",
    "                # convert to a dataframe for easier extraction\n",
    "                df = pd.read_json(json_path)\n",
    "                # Loop through the data to extract what we want\n",
    "                for data in df['data']['districts']:\n",
    "                    # Extract the District and append it to the appropriate list in the dictionary\n",
    "                    top_transact_district['District'].append(data['entityName'].title().replace(' And', ' and').replace('andaman', 'Andaman'))\n",
    "                    # Extract the Total Transaction and append it to the appropriate list in the dictionary\n",
    "                    top_transact_district['Total_Transaction'].append(data['metric']['count'])\n",
    "                    # Extract the Transaction Value and append it to the appropriate list in the dictionary\n",
    "                    top_transact_district['Transaction_Value'].append(data['metric']['amount'])\n",
    "                    # Here we retrieve the current state we're extracting from remove uneccesary symbols, clean and Capitalize then append it to the appropriate list in the dictionary\n",
    "                    top_transact_district['State'].append(state.title().replace('-', ' ').replace('&', 'and'))\n",
    "                    # Here we retrieve the year and append it to the appropriate list in the dictionary\n",
    "                    top_transact_district['Year'].append(year)\n",
    "                    # Reterieve the Quater we're curently extracting from and append it to the appropriate list in the dictionary\n",
    "                    top_transact_district['Quarter'].append(json_f)\n",
    "    # We convert the Dictionary into a Dataframe\n",
    "    df_top_district = pd.DataFrame(top_transact_district)\n",
    "    # Return the Dataframe\n",
    "    return df_top_district"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8f9eca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the function to extract Top Transaction Pincode data, this will accept the path to the Top Transaction\n",
    "\n",
    "def extract_top_transact_pincode_data(path):\n",
    "    #Create a dictionary which contains an empty list to store each items that will be retrieved\n",
    "    top_transact_pincode = {\n",
    "        'State':[], 'Year':[], 'Quarter':[],\n",
    "        'Pincode':[], 'Total_Transaction': [], 'Transaction_Value': []\n",
    "    }\n",
    "    # loop over each state in the states subfolder\n",
    "    for state in os.listdir(path):\n",
    "        # Add each state to the path\n",
    "        state_path = f\"{path}/{state}\"\n",
    "        # loop over each year in the state path derived above\n",
    "        for year in os.listdir(state_path):\n",
    "            # add each year to the path\n",
    "            year_path = f\"{state_path}/{year}\"\n",
    "            # loop over the json files\n",
    "            for json_file in os.listdir(year_path):\n",
    "                # add each file to the path\n",
    "                json_path = f\"{year_path}/{json_file}\"\n",
    "                # Retreive what quater we're currently extracting, by separating the .json handle\n",
    "                json_f = json_file.split('.json')[0]\n",
    "                # convert to a dataframe for easier extraction\n",
    "                df = pd.read_json(json_path)\n",
    "                # Loop through the data to extract what we want\n",
    "                for data in df['data']['pincodes']:\n",
    "                    # Extract the Pincode and append it to the appropriate list in the dictionary\n",
    "                    # But check if it's not empty\n",
    "                    if data['entityName'] is not None:\n",
    "                        # if it's not empty, we convert it to a string and append it to the appropriate list in the dictionary\n",
    "                        top_transact_pincode['Pincode'].append(data['entityName'].title())\n",
    "                    else:\n",
    "                        # if it's empty then we attach the empty string \n",
    "                        top_transact_pincode['Pincode'].append(data['entityName'])\n",
    "                    # Extract the Total Transaction and append it to the appropriate list in the dictionary\n",
    "                    top_transact_pincode['Total_Transaction'].append(data['metric']['count'])\n",
    "                    # Extract the Transaction Value and append it to the appropriate list in the dictionary\n",
    "                    top_transact_pincode['Transaction_Value'].append(data['metric']['amount'])\n",
    "                    # Here we retrieve the current state we're extracting from remove uneccesary symbols, clean and Capitalize then append it to the appropriate list in the dictionary\n",
    "                    top_transact_pincode['State'].append(state.title().replace('-', ' ').replace('&', 'and'))\n",
    "                    # Here we retrieve the year and append it to the appropriate list in the dictionary\n",
    "                    top_transact_pincode['Year'].append(year)\n",
    "                    # Reterieve the Quater we're curently extracting from and append it to the appropriate list in the dictionary\n",
    "                    top_transact_pincode['Quarter'].append(json_f)\n",
    "    # We convert the Dictionary into a Dataframe\n",
    "    df_top_pincode = pd.DataFrame(top_transact_pincode)\n",
    "    # Return the Dataframe\n",
    "    return df_top_pincode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713f36a3",
   "metadata": {},
   "source": [
    "**Top User Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "675eadcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the function to extract Top User District data, this will accept the path to the Top User\n",
    "\n",
    "def extract_top_user_district_data(path):\n",
    "    #Create a dictionary which contains an empty list to store each items that will be retrieved\n",
    "    top_user_district = {\n",
    "        'State':[], 'Year':[], 'Quarter':[],\n",
    "        'District':[], 'Registered_Users': []\n",
    "    }\n",
    "    # loop over each state in the states subfolder\n",
    "    for state in os.listdir(path):\n",
    "        # Add each state to the path\n",
    "        state_path = f\"{path}/{state}\"\n",
    "        # loop over each year in the state path derived above\n",
    "        for year in os.listdir(state_path):\n",
    "            # add each year to the path\n",
    "            year_path = f\"{state_path}/{year}\"\n",
    "            # loop over the json files\n",
    "            for json_file in os.listdir(year_path):\n",
    "                # add each file to the path\n",
    "                json_path = f\"{year_path}/{json_file}\"\n",
    "                # Retreive what quater we're currently extracting, by separating the .json handle\n",
    "                json_f = json_file.split('.json')[0]\n",
    "                # convert to a dataframe for easier extraction\n",
    "                df = pd.read_json(json_path)\n",
    "                # Loop through the data to extract what we want\n",
    "                for data in df['data']['districts']:\n",
    "                    # Extract the District and append it to the appropriate list in the dictionary\n",
    "                    top_user_district['District'].append(data['name'].title().replace(' And', ' and').replace('andaman', 'Andaman'))\n",
    "                    # Extract the Total registered user and append it to the appropriate list in the dictionary\n",
    "                    top_user_district['Registered_Users'].append(data['registeredUsers'])\n",
    "                    # Here we retrieve the current state we're extracting from remove uneccesary symbols, clean and Capitalize then append it to the appropriate list in the dictionary\n",
    "                    top_user_district['State'].append(state.title().replace('-', ' ').replace('&', 'and'))\n",
    "                    # Here we retrieve the year and append it to the appropriate list in the dictionary\n",
    "                    top_user_district['Year'].append(year)\n",
    "                    # Reterieve the Quater we're curently extracting from and append it to the appropriate list in the dictionary\n",
    "                    top_user_district['Quarter'].append(json_f)\n",
    "    # We convert the Dictionary into a Dataframe\n",
    "    df_top_user_district = pd.DataFrame(top_user_district)\n",
    "    # Return the Dataframe\n",
    "    return df_top_user_district"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77f9c8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the function to extract Top User District data, this will accept the path to the Top User\n",
    "\n",
    "def extract_top_user_pincode_data(path):\n",
    "    #Create a dictionary which contains an empty list to store each items that will be retrieved\n",
    "    top_user_pincode = {\n",
    "        'State':[], 'Year':[], 'Quarter':[],\n",
    "        'pincode':[], 'Registered_Users': []\n",
    "    }\n",
    "    # loop over each state in the states subfolder\n",
    "    for state in os.listdir(path):\n",
    "        # Add each state to the path\n",
    "        state_path = f\"{path}/{state}\"\n",
    "        # loop over each year in the state path derived above\n",
    "        for year in os.listdir(state_path):\n",
    "            # add each year to the path\n",
    "            year_path = f\"{state_path}/{year}\"\n",
    "            # loop over the json files\n",
    "            for json_file in os.listdir(year_path):\n",
    "                # add each file to the path\n",
    "                json_path = f\"{year_path}/{json_file}\"\n",
    "                # Retreive what quater we're currently extracting, by separating the .json handle\n",
    "                json_f = json_file.split('.json')[0]\n",
    "                # convert to a dataframe for easier extraction\n",
    "                df = pd.read_json(json_path)\n",
    "                # Loop through the data to extract what we want\n",
    "                for data in df['data']['pincodes']:\n",
    "                    # Extract the Pincode and append it to the appropriate list in the dictionary\n",
    "                    top_user_pincode['pincode'].append(data['name'].title())\n",
    "                    # Extract the Total registered user and append it to the appropriate list in the dictionary\n",
    "                    top_user_pincode['Registered_Users'].append(data['registeredUsers'])\n",
    "                    # Here we retrieve the current state we're extracting from remove uneccesary symbols, clean and Capitalize then append it to the appropriate list in the dictionary\n",
    "                    top_user_pincode['State'].append(state.title().replace('-', ' ').replace('&', 'and'))\n",
    "                    # Here we retrieve the year and append it to the appropriate list in the dictionary\n",
    "                    top_user_pincode['Year'].append(year)\n",
    "                    # Reterieve the Quater we're curently extracting from and append it to the appropriate list in the dictionary\n",
    "                    top_user_pincode['Quarter'].append(json_f)\n",
    "    # We convert the Dictionary into a Dataframe\n",
    "    df_top_user_pincode = pd.DataFrame(top_user_pincode)\n",
    "    # Return the Dataframe\n",
    "    return df_top_user_pincode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa724b6",
   "metadata": {},
   "source": [
    "***Extract The Dataframes***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abd38120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and Extract the dataframe\n",
    "\n",
    "# Now you call all functions to extract the data and convert to a dataframe\n",
    "agg_trans_df = extract_agg_transact_data(state_paths[0])\n",
    "agg_user_df = extract_agg_user_data(state_paths[1])\n",
    "map_trans_df = extract_map_transact_data(state_paths[2])\n",
    "map_user_df = extract_map_user_data(state_paths[3])\n",
    "top_trans_dist_df = extract_top_transact_district_data(state_paths[4])\n",
    "top_trans_pin_df = extract_top_transact_pincode_data(state_paths[4])\n",
    "top_user_dist_df = extract_top_user_district_data(state_paths[5])\n",
    "top_user_pin_df = extract_top_user_pincode_data(state_paths[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "990e4f5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['agg_trans_df',\n",
       " 'agg_user_df',\n",
       " 'map_trans_df',\n",
       " 'map_user_df',\n",
       " 'top_trans_dist_df',\n",
       " 'top_trans_pin_df',\n",
       " 'top_user_dist_df',\n",
       " 'top_user_pin_df']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Put all Dataframes names in a list so we can easily call back\n",
    "\n",
    "# To the the name of Df(dataframe), we get all items using the globals function, then we filte by checking if it's the instance of a Dataframe and is also ending with _df\n",
    "df_list = [var_name for var_name, var_value in globals().items() if isinstance(var_value, pd.DataFrame) and var_name.endswith('_df')]\n",
    "# Here's the list\n",
    "df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3c0f94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now it was noticed that most district in delhi didn't end with delhi i.e North delhi is North, and it's a bit confusing so we need to fix it. \n",
    "# To fix it, we will create a function that will loop through the data and go through the state of delhi and add suffix delhi to the district xcept Shahdara\n",
    "# Create  function that accepts the dataframe \n",
    "def add_delhi(df):\n",
    "    # First we check if State and Distircts are in the columns of the data frame, because thoseare the required columns for thi function to work\n",
    "    if 'State' in df.columns and 'District' in df.columns:\n",
    "        # First we strip the district off the district column, the replace it with nothing, convert it back to a string and set as the new distirct\n",
    "        df['District'] = df['District'].str.replace('District', '').str.strip()\n",
    "        # Now we select only data where the state is delhi\n",
    "        delhi_df = df[df['State'] == 'Delhi']\n",
    "        # Now we create a list of the unique District but we're exempting Shahdara\n",
    "        districts_to_suffix = [d for d in delhi_df['District'].unique() if d != 'Shahdara']\n",
    "        # Now we check for the distirct then change it with the district ending with delhi\n",
    "        df.loc[(df['State'] == 'Delhi') & (df['District'].isin(districts_to_suffix)), 'District'] = df.loc[(df['State'] == 'Delhi') & (df['District'].isin(districts_to_suffix)), 'District'].apply(lambda x: x + ' Delhi' if 'Delhi' not in x else x)\n",
    "    # Now return the new data frame\n",
    "    return df\n",
    "# Loop over the Dataframe list\n",
    "for name in df_list:\n",
    "    # Convert to a Dataframe \n",
    "    df = globals()[name]\n",
    "    # Now call add delhi function\n",
    "    add_delhi(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b0fd63",
   "metadata": {},
   "source": [
    "Now we need to add a geo location data, so i sourced out the data on india on the internet. And we will merge it with our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e1927d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After converting the data into a CSV file, the we convert it into a dataframe \n",
    "Longitude_and_latitude_df = pd.read_csv('csv_files/dist_lat_long.csv')\n",
    "\n",
    "# Loop over the Dataframe list\n",
    "for name in df_list:\n",
    "    # Convert to a Dataframe \n",
    "    df = globals()[name]\n",
    "    # First we check if Districts is in the data frame\n",
    "    if 'District' in df.columns:\n",
    "        # Then we merge the current Dataframe with the longitud dataframe on the state and district column\n",
    "        df = pd.merge(df, Longitude_and_latitude_df, on=['State', 'District'])\n",
    "        # Assign the dataframe to the to the global dataframe \n",
    "        globals()[name] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09975c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to signify the region using the states, to do that we will need to creat a function to add the region\n",
    "\n",
    "# Create  function that accepts the dataframe\n",
    "def add_region(df):\n",
    "    # Now we have create a dictionary with the region as a key an the list of states in it\n",
    "    state_groups = {\n",
    "        'Northern Region': ['Jammu and Kashmir', 'Himachal Pradesh', 'Punjab', 'Chandigarh', 'Uttarakhand', 'Ladakh', 'Delhi', 'Haryana'],\n",
    "        'Central Region': ['Uttar Pradesh', 'Madhya Pradesh', 'Chhattisgarh'],\n",
    "        'Western Region': ['Rajasthan', 'Gujarat', 'Dadra and Nagar Haveli and Daman and Diu', 'Maharashtra'],\n",
    "        'Eastern Region': ['Bihar', 'Jharkhand', 'Odisha', 'West Bengal', 'Sikkim'],\n",
    "        'Southern Region': ['Andhra Pradesh', 'Telangana', 'Karnataka', 'Kerala', 'Tamil Nadu', 'Puducherry', 'Goa', 'Lakshadweep', 'Andaman and Nicobar Islands'],\n",
    "        'North-Eastern Region': ['Assam', 'Meghalaya', 'Manipur', 'Nagaland', 'Tripura', 'Arunachal Pradesh', 'Mizoram']\n",
    "    }\n",
    "\n",
    "    # Loop throught the dictionary\n",
    "    for region, states in state_groups.items():\n",
    "        # Now we check if the state in the region then we add that region on a new column, Region\n",
    "        df.loc[df['State'].isin(states), 'Region'] = region\n",
    "    # Now you return the New Dataframe\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58537bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over the Dataframe list\n",
    "for name in df_list:\n",
    "    # Convert to a Dataframe \n",
    "    df = globals()[name]\n",
    "    # Now add the regio using the function create earlier\n",
    "    add_region(df)\n",
    "    # Assign the dataframe to the to the global dataframe \n",
    "    globals()[name] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a870587f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agg_trans_df:\n",
      "_________________________\n",
      "State                0\n",
      "Year                 0\n",
      "Quarter              0\n",
      "Payment_Type         0\n",
      "Total_Transaction    0\n",
      "Transaction_Value    0\n",
      "Region               0\n",
      "dtype: int64\n",
      "Duplicated Rows: 0\n",
      "Dataframe Shape: (3594, 7)\n",
      "*************************\n",
      "agg_user_df:\n",
      "_________________________\n",
      "State                             0\n",
      "Year                              0\n",
      "Quarter                           0\n",
      "Registered_user                   0\n",
      "App_Open                          0\n",
      "Phone_Brand                       0\n",
      "Registered_user_by_Phone_Brand    0\n",
      "Region                            0\n",
      "dtype: int64\n",
      "Duplicated Rows: 0\n",
      "Dataframe Shape: (6732, 8)\n",
      "*************************\n",
      "map_trans_df:\n",
      "_________________________\n",
      "State                0\n",
      "Year                 0\n",
      "Quarter              0\n",
      "District             0\n",
      "Total_Transaction    0\n",
      "Transaction_Value    0\n",
      "Latitude             0\n",
      "Longitude            0\n",
      "Region               0\n",
      "dtype: int64\n",
      "Duplicated Rows: 0\n",
      "Dataframe Shape: (14636, 9)\n",
      "*************************\n",
      "map_user_df:\n",
      "_________________________\n",
      "State                    0\n",
      "Year                     0\n",
      "Quarter                  0\n",
      "District                 0\n",
      "Total_Registered_User    0\n",
      "Total_App_open           0\n",
      "Latitude                 0\n",
      "Longitude                0\n",
      "Region                   0\n",
      "dtype: int64\n",
      "Duplicated Rows: 0\n",
      "Dataframe Shape: (14640, 9)\n",
      "*************************\n",
      "top_trans_dist_df:\n",
      "_________________________\n",
      "State                0\n",
      "Year                 0\n",
      "Quarter              0\n",
      "District             0\n",
      "Total_Transaction    0\n",
      "Transaction_Value    0\n",
      "Latitude             0\n",
      "Longitude            0\n",
      "Region               0\n",
      "dtype: int64\n",
      "Duplicated Rows: 0\n",
      "Dataframe Shape: (5920, 9)\n",
      "*************************\n",
      "top_trans_pin_df:\n",
      "_________________________\n",
      "State                0\n",
      "Year                 0\n",
      "Quarter              0\n",
      "Pincode              2\n",
      "Total_Transaction    0\n",
      "Transaction_Value    0\n",
      "Region               0\n",
      "dtype: int64\n",
      "Duplicated Rows: 0\n",
      "Dataframe Shape: (7139, 7)\n",
      "*************************\n",
      "top_user_dist_df:\n",
      "_________________________\n",
      "State               0\n",
      "Year                0\n",
      "Quarter             0\n",
      "District            0\n",
      "Registered_Users    0\n",
      "Latitude            0\n",
      "Longitude           0\n",
      "Region              0\n",
      "dtype: int64\n",
      "Duplicated Rows: 0\n",
      "Dataframe Shape: (5920, 8)\n",
      "*************************\n",
      "top_user_pin_df:\n",
      "_________________________\n",
      "State               0\n",
      "Year                0\n",
      "Quarter             0\n",
      "pincode             0\n",
      "Registered_Users    0\n",
      "Region              0\n",
      "dtype: int64\n",
      "Duplicated Rows: 0\n",
      "Dataframe Shape: (7140, 6)\n",
      "*************************\n"
     ]
    }
   ],
   "source": [
    "# Now we have gotten the datafrmes ready, then we check for null alues ambiguities etc...\n",
    "\n",
    "# Loop over the Dataframe list\n",
    "for name in df_list:\n",
    "    # Convert to a Dataframe\n",
    "    df = globals()[name]\n",
    "    # Get the dataframe name\n",
    "    print(f'{name}:')\n",
    "    # add a seperator\n",
    "    print(25 * '_')\n",
    "    # Now get the Null Values\n",
    "    print(f'{df.isnull().sum()}')\n",
    "    # Now check for duplicated values\n",
    "    print(f'Duplicated Rows: {df.duplicated().sum()}')\n",
    "    # Also print out the shapes, row and column\n",
    "    print(f'Dataframe Shape: {df.shape}')\n",
    "    # Add a separator\n",
    "    print(25 * '*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "712d1869",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "State                0\n",
       "Year                 0\n",
       "Quarter              0\n",
       "Pincode              0\n",
       "Total_Transaction    0\n",
       "Transaction_Value    0\n",
       "Region               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I noticed that top_trans_pin_df has pincode with 2 missing data, so i nee to remove it \n",
    "\n",
    "# Firstly, since its insignificant then we remove those rows \n",
    "top_trans_pin_df.dropna(subset=['Pincode'], inplace=True)\n",
    "\n",
    "# Then we check again\n",
    "top_trans_pin_df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "730457d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe Info: \n",
      "\n",
      "agg_trans_df\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3594 entries, 0 to 3593\n",
      "Data columns (total 7 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   State              3594 non-null   object \n",
      " 1   Year               3594 non-null   object \n",
      " 2   Quarter            3594 non-null   object \n",
      " 3   Payment_Type       3594 non-null   object \n",
      " 4   Total_Transaction  3594 non-null   int64  \n",
      " 5   Transaction_Value  3594 non-null   float64\n",
      " 6   Region             3594 non-null   object \n",
      "dtypes: float64(1), int64(1), object(5)\n",
      "memory usage: 196.7+ KB\n",
      "\n",
      " ********************************************* \n",
      "\n",
      "agg_user_df\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6732 entries, 0 to 6731\n",
      "Data columns (total 8 columns):\n",
      " #   Column                          Non-Null Count  Dtype \n",
      "---  ------                          --------------  ----- \n",
      " 0   State                           6732 non-null   object\n",
      " 1   Year                            6732 non-null   object\n",
      " 2   Quarter                         6732 non-null   object\n",
      " 3   Registered_user                 6732 non-null   int64 \n",
      " 4   App_Open                        6732 non-null   int64 \n",
      " 5   Phone_Brand                     6732 non-null   object\n",
      " 6   Registered_user_by_Phone_Brand  6732 non-null   int64 \n",
      " 7   Region                          6732 non-null   object\n",
      "dtypes: int64(3), object(5)\n",
      "memory usage: 420.9+ KB\n",
      "\n",
      " ********************************************* \n",
      "\n",
      "map_trans_df\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14636 entries, 0 to 14635\n",
      "Data columns (total 9 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   State              14636 non-null  object \n",
      " 1   Year               14636 non-null  object \n",
      " 2   Quarter            14636 non-null  object \n",
      " 3   District           14636 non-null  object \n",
      " 4   Total_Transaction  14636 non-null  int64  \n",
      " 5   Transaction_Value  14636 non-null  float64\n",
      " 6   Latitude           14636 non-null  float64\n",
      " 7   Longitude          14636 non-null  float64\n",
      " 8   Region             14636 non-null  object \n",
      "dtypes: float64(3), int64(1), object(5)\n",
      "memory usage: 1.0+ MB\n",
      "\n",
      " ********************************************* \n",
      "\n",
      "map_user_df\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14640 entries, 0 to 14639\n",
      "Data columns (total 9 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   State                  14640 non-null  object \n",
      " 1   Year                   14640 non-null  object \n",
      " 2   Quarter                14640 non-null  object \n",
      " 3   District               14640 non-null  object \n",
      " 4   Total_Registered_User  14640 non-null  int64  \n",
      " 5   Total_App_open         14640 non-null  int64  \n",
      " 6   Latitude               14640 non-null  float64\n",
      " 7   Longitude              14640 non-null  float64\n",
      " 8   Region                 14640 non-null  object \n",
      "dtypes: float64(2), int64(2), object(5)\n",
      "memory usage: 1.0+ MB\n",
      "\n",
      " ********************************************* \n",
      "\n",
      "top_trans_dist_df\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5920 entries, 0 to 5919\n",
      "Data columns (total 9 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   State              5920 non-null   object \n",
      " 1   Year               5920 non-null   object \n",
      " 2   Quarter            5920 non-null   object \n",
      " 3   District           5920 non-null   object \n",
      " 4   Total_Transaction  5920 non-null   int64  \n",
      " 5   Transaction_Value  5920 non-null   float64\n",
      " 6   Latitude           5920 non-null   float64\n",
      " 7   Longitude          5920 non-null   float64\n",
      " 8   Region             5920 non-null   object \n",
      "dtypes: float64(3), int64(1), object(5)\n",
      "memory usage: 416.4+ KB\n",
      "\n",
      " ********************************************* \n",
      "\n",
      "top_trans_pin_df\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 7137 entries, 0 to 7138\n",
      "Data columns (total 7 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   State              7137 non-null   object \n",
      " 1   Year               7137 non-null   object \n",
      " 2   Quarter            7137 non-null   object \n",
      " 3   Pincode            7137 non-null   object \n",
      " 4   Total_Transaction  7137 non-null   int64  \n",
      " 5   Transaction_Value  7137 non-null   float64\n",
      " 6   Region             7137 non-null   object \n",
      "dtypes: float64(1), int64(1), object(5)\n",
      "memory usage: 446.1+ KB\n",
      "\n",
      " ********************************************* \n",
      "\n",
      "top_user_dist_df\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5920 entries, 0 to 5919\n",
      "Data columns (total 8 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   State             5920 non-null   object \n",
      " 1   Year              5920 non-null   object \n",
      " 2   Quarter           5920 non-null   object \n",
      " 3   District          5920 non-null   object \n",
      " 4   Registered_Users  5920 non-null   int64  \n",
      " 5   Latitude          5920 non-null   float64\n",
      " 6   Longitude         5920 non-null   float64\n",
      " 7   Region            5920 non-null   object \n",
      "dtypes: float64(2), int64(1), object(5)\n",
      "memory usage: 370.1+ KB\n",
      "\n",
      " ********************************************* \n",
      "\n",
      "top_user_pin_df\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7140 entries, 0 to 7139\n",
      "Data columns (total 6 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   State             7140 non-null   object\n",
      " 1   Year              7140 non-null   object\n",
      " 2   Quarter           7140 non-null   object\n",
      " 3   pincode           7140 non-null   object\n",
      " 4   Registered_Users  7140 non-null   int64 \n",
      " 5   Region            7140 non-null   object\n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 334.8+ KB\n",
      "\n",
      " ********************************************* \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now lets check for the Dataframe information\n",
    "print('Dataframe Info: \\n')\n",
    "# Loop over the Dataframe list\n",
    "for name in df_list:\n",
    "    # Convert to a Dataframe\n",
    "    df = globals()[name]\n",
    "    # print the name of the dataframe\n",
    "    print(f'{name}')\n",
    "    # Print out the information for the Data frame\n",
    "    df.info()\n",
    "    # add a separator\n",
    "    print(f'\\n', 45 * '*', '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4d50f308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we see that year appars as an object instead of beinf and int, we solve that\n",
    "# Loop over the Dataframe list\n",
    "for name in df_list:\n",
    "    # Convert to a Dataframe\n",
    "    df = globals()[name]\n",
    "    # Convert year to an int type\n",
    "    df['Year'] = df['Year'].astype('int')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c786a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe Info: \n",
      "\n",
      "agg_trans_df\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3594 entries, 0 to 3593\n",
      "Data columns (total 7 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   State              3594 non-null   object \n",
      " 1   Year               3594 non-null   int32  \n",
      " 2   Quarter            3594 non-null   object \n",
      " 3   Payment_Type       3594 non-null   object \n",
      " 4   Total_Transaction  3594 non-null   int64  \n",
      " 5   Transaction_Value  3594 non-null   float64\n",
      " 6   Region             3594 non-null   object \n",
      "dtypes: float64(1), int32(1), int64(1), object(4)\n",
      "memory usage: 182.6+ KB\n",
      "\n",
      " ********************************************* \n",
      "\n",
      "agg_user_df\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6732 entries, 0 to 6731\n",
      "Data columns (total 8 columns):\n",
      " #   Column                          Non-Null Count  Dtype \n",
      "---  ------                          --------------  ----- \n",
      " 0   State                           6732 non-null   object\n",
      " 1   Year                            6732 non-null   int32 \n",
      " 2   Quarter                         6732 non-null   object\n",
      " 3   Registered_user                 6732 non-null   int64 \n",
      " 4   App_Open                        6732 non-null   int64 \n",
      " 5   Phone_Brand                     6732 non-null   object\n",
      " 6   Registered_user_by_Phone_Brand  6732 non-null   int64 \n",
      " 7   Region                          6732 non-null   object\n",
      "dtypes: int32(1), int64(3), object(4)\n",
      "memory usage: 394.6+ KB\n",
      "\n",
      " ********************************************* \n",
      "\n",
      "map_trans_df\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14636 entries, 0 to 14635\n",
      "Data columns (total 9 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   State              14636 non-null  object \n",
      " 1   Year               14636 non-null  int32  \n",
      " 2   Quarter            14636 non-null  object \n",
      " 3   District           14636 non-null  object \n",
      " 4   Total_Transaction  14636 non-null  int64  \n",
      " 5   Transaction_Value  14636 non-null  float64\n",
      " 6   Latitude           14636 non-null  float64\n",
      " 7   Longitude          14636 non-null  float64\n",
      " 8   Region             14636 non-null  object \n",
      "dtypes: float64(3), int32(1), int64(1), object(4)\n",
      "memory usage: 972.0+ KB\n",
      "\n",
      " ********************************************* \n",
      "\n",
      "map_user_df\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14640 entries, 0 to 14639\n",
      "Data columns (total 9 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   State                  14640 non-null  object \n",
      " 1   Year                   14640 non-null  int32  \n",
      " 2   Quarter                14640 non-null  object \n",
      " 3   District               14640 non-null  object \n",
      " 4   Total_Registered_User  14640 non-null  int64  \n",
      " 5   Total_App_open         14640 non-null  int64  \n",
      " 6   Latitude               14640 non-null  float64\n",
      " 7   Longitude              14640 non-null  float64\n",
      " 8   Region                 14640 non-null  object \n",
      "dtypes: float64(2), int32(1), int64(2), object(4)\n",
      "memory usage: 972.3+ KB\n",
      "\n",
      " ********************************************* \n",
      "\n",
      "top_trans_dist_df\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5920 entries, 0 to 5919\n",
      "Data columns (total 9 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   State              5920 non-null   object \n",
      " 1   Year               5920 non-null   int32  \n",
      " 2   Quarter            5920 non-null   object \n",
      " 3   District           5920 non-null   object \n",
      " 4   Total_Transaction  5920 non-null   int64  \n",
      " 5   Transaction_Value  5920 non-null   float64\n",
      " 6   Latitude           5920 non-null   float64\n",
      " 7   Longitude          5920 non-null   float64\n",
      " 8   Region             5920 non-null   object \n",
      "dtypes: float64(3), int32(1), int64(1), object(4)\n",
      "memory usage: 393.2+ KB\n",
      "\n",
      " ********************************************* \n",
      "\n",
      "top_trans_pin_df\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 7137 entries, 0 to 7138\n",
      "Data columns (total 7 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   State              7137 non-null   object \n",
      " 1   Year               7137 non-null   int32  \n",
      " 2   Quarter            7137 non-null   object \n",
      " 3   Pincode            7137 non-null   object \n",
      " 4   Total_Transaction  7137 non-null   int64  \n",
      " 5   Transaction_Value  7137 non-null   float64\n",
      " 6   Region             7137 non-null   object \n",
      "dtypes: float64(1), int32(1), int64(1), object(4)\n",
      "memory usage: 418.2+ KB\n",
      "\n",
      " ********************************************* \n",
      "\n",
      "top_user_dist_df\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5920 entries, 0 to 5919\n",
      "Data columns (total 8 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   State             5920 non-null   object \n",
      " 1   Year              5920 non-null   int32  \n",
      " 2   Quarter           5920 non-null   object \n",
      " 3   District          5920 non-null   object \n",
      " 4   Registered_Users  5920 non-null   int64  \n",
      " 5   Latitude          5920 non-null   float64\n",
      " 6   Longitude         5920 non-null   float64\n",
      " 7   Region            5920 non-null   object \n",
      "dtypes: float64(2), int32(1), int64(1), object(4)\n",
      "memory usage: 347.0+ KB\n",
      "\n",
      " ********************************************* \n",
      "\n",
      "top_user_pin_df\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7140 entries, 0 to 7139\n",
      "Data columns (total 6 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   State             7140 non-null   object\n",
      " 1   Year              7140 non-null   int32 \n",
      " 2   Quarter           7140 non-null   object\n",
      " 3   pincode           7140 non-null   object\n",
      " 4   Registered_Users  7140 non-null   int64 \n",
      " 5   Region            7140 non-null   object\n",
      "dtypes: int32(1), int64(1), object(4)\n",
      "memory usage: 306.9+ KB\n",
      "\n",
      " ********************************************* \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now lets check for the Dataframe information\n",
    "print('Dataframe Info: \\n')\n",
    "# Loop over the Dataframe list\n",
    "for name in df_list:\n",
    "    # Convert to a Dataframe\n",
    "    df = globals()[name]\n",
    "    # print the name of the dataframe\n",
    "    print(f'{name}')\n",
    "    # Print out the information for the Data frame\n",
    "    df.info()\n",
    "    # add a separator\n",
    "    print(f'\\n', 45 * '*', '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ab2933c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to check for outliers on all data frames, so we need a function that will count the outliers.\n",
    "\n",
    "# Create  function that accepts the dataframe\n",
    "def count_outliers(df):\n",
    "    # Create an empty dictionary to hold each column and the number of outliers they have\n",
    "    outliers = {}\n",
    "    # We loop throgh the dataframe's columns which has numbers in it, meaning int, float etc.\n",
    "    for col in df.select_dtypes(include=[np.number]).columns:\n",
    "        # Now we check if the column is either a Total Transaction or a Transaction Value\n",
    "        if col in ['Total_Transaction', 'Transaction_Value']:\n",
    "            # If it's either of the two columns then we proceed\n",
    "            # Now we calculate the first quantile\n",
    "            q1 = df[col].quantile(0.25)\n",
    "            # Now we calculate the third quantile\n",
    "            q3 = df[col].quantile(0.75)\n",
    "            # Then we proceed to calculate the interquatile range\n",
    "            iqr = q3 - q1\n",
    "            # Now we calculate the upper and lower bound\n",
    "            # Calculate the Upper bound\n",
    "            upper_bound = q3 + (1.5 * iqr)\n",
    "            # Calculate the Lower bound\n",
    "            lower_bound = q1 - (1.5 * iqr)\n",
    "            # Now we attach to the dictionary, the lenght of the data greater than the upper bound and lesser than the lower bound\n",
    "            outliers[col] = len(df[(df[col] > upper_bound) | (df[col] < lower_bound)])\n",
    "        # If it's neither of the two columns then we proceed\n",
    "        else:\n",
    "            # Continue\n",
    "            continue\n",
    "    # Now we return the dictionary\n",
    "    return outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a1300ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agg_trans_df :\n",
      "\n",
      " {'Total_Transaction': 652, 'Transaction_Value': 660} \n",
      "\n",
      "\n",
      " _______________________________________________________ \n",
      "\n",
      "map_trans_df :\n",
      "\n",
      " {'Total_Transaction': 1811, 'Transaction_Value': 1771} \n",
      "\n",
      "\n",
      " _______________________________________________________ \n",
      "\n",
      "top_trans_dist_df :\n",
      "\n",
      " {'Total_Transaction': 734, 'Transaction_Value': 743} \n",
      "\n",
      "\n",
      " _______________________________________________________ \n",
      "\n",
      "top_trans_pin_df :\n",
      "\n",
      " {'Total_Transaction': 999, 'Transaction_Value': 995} \n",
      "\n",
      "\n",
      " _______________________________________________________ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loop over the Dataframe list\n",
    "for name in df_list:\n",
    "    # Convert to a Dataframe\n",
    "    df = globals()[name]\n",
    "    # Now we run the function to get the count of our outliers and assign to a vairable\n",
    "    outliers = count_outliers(df)\n",
    "    # Now we check if th outlier isn't equal to zero\n",
    "    if len(outliers) == 0:\n",
    "        # If it is we pass\n",
    "        pass\n",
    "    else:\n",
    "        # If it's not, then we print out the name of the data frame and then the outliers\n",
    "        print(name, \":\\n\\n\", outliers, \"\\n\")\n",
    "        # add a separator\n",
    "        print(\"\\n\", 55 * \"_\", \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36408f1f",
   "metadata": {},
   "source": [
    "After checking for the outliers then we noticed that 4 DF's have outliers. Then we proceed to check for unique values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1375b1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to check for unique variables, so we will write a function to print the unique values\n",
    "\n",
    "# Create  function that accepts the dataframe, and excluded column\n",
    "def unique_values(df, exclude_cols=[]):\n",
    "    # Loop throught the dataframe's column\n",
    "    for col in df.columns:\n",
    "        # Check if the column is in the excluded colun list\n",
    "        if col in exclude_cols:\n",
    "            # If it is, then we we pass\n",
    "            continue\n",
    "        # If not then we proceed to extract the unique values\n",
    "        else:\n",
    "            #First, we get the unique values in the number of unique values in the column\n",
    "            unique_vals = df[col].nunique()\n",
    "            # Then we print the column followed by the number of unique values\n",
    "            print(f\"{col}: {unique_vals} unique values\")\n",
    "            # Now, check If the uniuqe value is lesser than 10\n",
    "            if unique_vals < 10:\n",
    "                # If it is then print the unique values\n",
    "                print(df[col].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "65459aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Values in Accross the Data Frame: \n",
      "\n",
      "agg_trans_df: \n",
      "\n",
      "Payment_Type: 5 unique values\n",
      "['Recharge & bill payments' 'Peer-to-peer payments' 'Merchant payments'\n",
      " 'Financial Services' 'Others']\n",
      "Total_Transaction: 3548 unique values\n",
      "Transaction_Value: 3594 unique values\n",
      "Region: 6 unique values\n",
      "['Southern Region' 'North-Eastern Region' 'Eastern Region'\n",
      " 'Northern Region' 'Central Region' 'Western Region']\n",
      "\n",
      " ******************************************************* \n",
      "\n",
      "agg_user_df: \n",
      "\n",
      "Registered_user: 612 unique values\n",
      "App_Open: 433 unique values\n",
      "Phone_Brand: 20 unique values\n",
      "Registered_user_by_Phone_Brand: 6501 unique values\n",
      "Region: 6 unique values\n",
      "['Southern Region' 'North-Eastern Region' 'Eastern Region'\n",
      " 'Northern Region' 'Central Region' 'Western Region']\n",
      "\n",
      " ******************************************************* \n",
      "\n",
      "map_trans_df: \n",
      "\n",
      "District: 727 unique values\n",
      "Total_Transaction: 14566 unique values\n",
      "Transaction_Value: 14636 unique values\n",
      "Latitude: 532 unique values\n",
      "Longitude: 540 unique values\n",
      "Region: 6 unique values\n",
      "['Southern Region' 'North-Eastern Region' 'Eastern Region'\n",
      " 'Northern Region' 'Central Region' 'Western Region']\n",
      "\n",
      " ******************************************************* \n",
      "\n",
      "map_user_df: \n",
      "\n",
      "District: 727 unique values\n",
      "Total_Registered_User: 14351 unique values\n",
      "Total_App_open: 14351 unique values\n",
      "Latitude: 532 unique values\n",
      "Longitude: 540 unique values\n",
      "Region: 6 unique values\n",
      "['Southern Region' 'North-Eastern Region' 'Eastern Region'\n",
      " 'Northern Region' 'Central Region' 'Western Region']\n",
      "\n",
      " ******************************************************* \n",
      "\n",
      "top_trans_dist_df: \n",
      "\n",
      "District: 368 unique values\n",
      "Total_Transaction: 5910 unique values\n",
      "Transaction_Value: 5920 unique values\n",
      "Latitude: 300 unique values\n",
      "Longitude: 299 unique values\n",
      "Region: 6 unique values\n",
      "['Southern Region' 'North-Eastern Region' 'Eastern Region'\n",
      " 'Northern Region' 'Central Region' 'Western Region']\n",
      "\n",
      " ******************************************************* \n",
      "\n",
      "top_trans_pin_df: \n",
      "\n",
      "Pincode: 746 unique values\n",
      "Total_Transaction: 7083 unique values\n",
      "Transaction_Value: 7137 unique values\n",
      "Region: 6 unique values\n",
      "['Southern Region' 'North-Eastern Region' 'Eastern Region'\n",
      " 'Northern Region' 'Central Region' 'Western Region']\n",
      "\n",
      " ******************************************************* \n",
      "\n",
      "top_user_dist_df: \n",
      "\n",
      "District: 313 unique values\n",
      "Registered_Users: 5874 unique values\n",
      "Latitude: 258 unique values\n",
      "Longitude: 258 unique values\n",
      "Region: 6 unique values\n",
      "['Southern Region' 'North-Eastern Region' 'Eastern Region'\n",
      " 'Northern Region' 'Central Region' 'Western Region']\n",
      "\n",
      " ******************************************************* \n",
      "\n",
      "top_user_pin_df: \n",
      "\n",
      "pincode: 426 unique values\n",
      "Registered_Users: 6882 unique values\n",
      "Region: 6 unique values\n",
      "['Southern Region' 'North-Eastern Region' 'Eastern Region'\n",
      " 'Northern Region' 'Central Region' 'Western Region']\n",
      "\n",
      " ******************************************************* \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now lets check the unique values of all data frames\n",
    "\n",
    "# Let's print the tittle\n",
    "print('Unique Values in Accross the Data Frame: \\n')\n",
    "\n",
    "# Loop over the Dataframe list\n",
    "for name in df_list:\n",
    "    # Convert to a Dataframe\n",
    "    df = globals()[name]\n",
    "    # Print the name\n",
    "    print(f'{name}: \\n')\n",
    "    # Call the unique values function\n",
    "    unique_values(df, exclude_cols=['State', 'Year', 'Quarter'])\n",
    "    # add a separator\n",
    "    print(\"\\n\", 55 * \"*\", \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e8f71132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's save all data frames, converting it into a csv file\n",
    "\n",
    "# Create  function that accepts the dataframe list\n",
    "def save_csv(df_list):\n",
    "    # Select the subfolders where you want the files to be \n",
    "    subfolder = \"C:/Users/a/OneDrive/Desktop/Github_projects/DS_Python_projects/Dashboard Project/csv_files\"\n",
    "    # Now if the subfolder path dosen't exist\n",
    "    if not os.path.exists(subfolder):\n",
    "        # If it's not then create it\n",
    "        os.makedirs(subfolder) \n",
    "        # loop over the list \n",
    "    for df_name in df_list:\n",
    "        # Convert to a Dataframe\n",
    "        df = globals()[df_name]\n",
    "        #Create a file path, by joining the subfolder path with the derived file name ending with CSV\n",
    "        file_path = os.path.join(subfolder, df_name.replace('_df', '') + '.csv')\n",
    "        # Convert to  CSV file using the path\n",
    "        df.to_csv(file_path, index=False)\n",
    "    # Print a succesful message \n",
    "    print('Successful')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5de6635a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful\n"
     ]
    }
   ],
   "source": [
    "# Call the function \n",
    "save_csv(df_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b854810",
   "metadata": {},
   "source": [
    "Now we have been able to Extract, clean and prepae our data. Now we need to push it into our database for easy extraction and accesibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "83756eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please Enter your Password: \n",
      "Password: \n"
     ]
    }
   ],
   "source": [
    "# First we need to connect to our database, and to that we will need our loggin details. But because those details are sensitive we will input them using get pass\n",
    "# First we get credentials, we already know the username we now need to get the username\n",
    "# Print prompt\n",
    "print('Please Enter your Password: ')\n",
    "\n",
    "# Now to secure the password we use get pass to encode it\n",
    "my_password = getpass(\"Password: \")\n",
    "\n",
    "#Now we have the credentials, let's connect to the database using the Mysql collector\n",
    "conn = mysql.connector.connect(\n",
    "  # Get the host\n",
    "  host = \"localhost\",\n",
    "  # Get the Username\n",
    "  user = \"root\",\n",
    "  # Get the password\n",
    "  password = f\"{my_password}\"\n",
    ")\n",
    "\n",
    "# Now we connect to our cursor\n",
    "cursor = conn.cursor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1e8ae35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's drop th database if it exist on the Database\n",
    "cursor.execute(\"DROP DATABASE IF EXISTS phone_pulse\")\n",
    "# Now We create the database\n",
    "cursor.execute(\"CREATE DATABASE phone_pulse\")\n",
    "# Then we need to select the newly created database \n",
    "cursor.execute(\"USE phone_pulse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d0181824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we create Tables for all dataframes on our database\n",
    "cursor.execute('''CREATE TABLE agg_trans (\n",
    "                    State VARCHAR(250),\n",
    "                    Year YEAR,\n",
    "                    Quarter INTEGER,\n",
    "                    Payment_Type VARCHAR(250),\n",
    "                    Total_Transaction INTEGER,\n",
    "                    Transaction_Value FLOAT,\n",
    "                    Region VARCHAR(250),\n",
    "                    PRIMARY KEY (State(250), Year, Quarter, Payment_type(250), Region(250))\n",
    "                )''')\n",
    "cursor.execute('''CREATE TABLE agg_user (\n",
    "                    State VARCHAR(250), \n",
    "                    Year YEAR, \n",
    "                    Quarter INTEGER, \n",
    "                    Registered_user INTEGER, \n",
    "                    App_open BIGINT, \n",
    "                    Phone_Brand VARCHAR(250), \n",
    "                    Registered_user_by_Phone_Brand INTEGER, \n",
    "                    Region VARCHAR(250), \n",
    "                    PRIMARY KEY (State(250), Year, Quarter, Phone_Brand(250), Region(250))\n",
    "                )''')\n",
    "cursor.execute('''CREATE TABLE map_trans (\n",
    "                    State VARCHAR(250), \n",
    "                    Year YEAR, \n",
    "                    Quarter INTEGER, \n",
    "                    District VARCHAR(250), \n",
    "                    Total_Transaction INTEGER, \n",
    "                    Transaction_Value FLOAT, \n",
    "                    Latitude FLOAT, \n",
    "                    Longitude FLOAT, \n",
    "                    Region VARCHAR(250),\n",
    "                    PRIMARY KEY (State(250), Year, Quarter, District(250), Region(250))\n",
    "                )''')\n",
    "cursor.execute('''CREATE TABLE map_user (\n",
    "                    State VARCHAR(250), \n",
    "                    Year YEAR, \n",
    "                    Quarter INTEGER, \n",
    "                    District VARCHAR(250), \n",
    "                    Total_Registered_User INTEGER, \n",
    "                    Total_App_open INTEGER, \n",
    "                    Latitude FLOAT, \n",
    "                    Longitude FLOAT, \n",
    "                    Region VARCHAR(250), \n",
    "                    PRIMARY KEY (State(250), Year, Quarter, District(250), Region(250))\n",
    "                )''')\n",
    "cursor.execute('''CREATE TABLE top_trans_dist (\n",
    "                    State VARCHAR(250), \n",
    "                    Year YEAR, \n",
    "                    Quarter INTEGER, \n",
    "                    District VARCHAR(250), \n",
    "                    Total_Transaction INTEGER, \n",
    "                    Transaction_Value FLOAT, \n",
    "                    Latitude FLOAT, \n",
    "                    Longitude FLOAT, \n",
    "                    Region VARCHAR(250),\n",
    "                    PRIMARY KEY (State(250), Year, Quarter, District(250), Region(250))\n",
    "                )''')\n",
    "cursor.execute('''CREATE TABLE top_trans_pin (\n",
    "                    State VARCHAR(250), \n",
    "                    Year YEAR, \n",
    "                    Quarter INTEGER, \n",
    "                    Pincode VARCHAR(250), \n",
    "                    Total_Transaction INTEGER, \n",
    "                    Transaction_Value FLOAT, \n",
    "                    Region VARCHAR(250),\n",
    "                    PRIMARY KEY (State(250), Year, Quarter, Pincode(250), Region(250))\n",
    "                )''')\n",
    "cursor.execute('''CREATE TABLE top_user_dist (\n",
    "                    State VARCHAR(250), \n",
    "                    Year YEAR, \n",
    "                    Quarter INTEGER, \n",
    "                    District VARCHAR(250), \n",
    "                    Registered_Users INTEGER, \n",
    "                    Latitude FLOAT, \n",
    "                    Longitude FLOAT, \n",
    "                    Region VARCHAR(250),\n",
    "                    PRIMARY KEY (State(250), Year, Quarter, District(250), Region(250))\n",
    "                )''')\n",
    "cursor.execute('''CREATE TABLE top_user_pin (\n",
    "                    State VARCHAR(250), \n",
    "                    Year YEAR, \n",
    "                    Quarter INTEGER, \n",
    "                    Pincode VARCHAR(250), \n",
    "                    Registered_Users INTEGER, \n",
    "                    Region VARCHAR(250),\n",
    "                    PRIMARY KEY (State(250), Year, Quarter, Pincode(250), Region(250))\n",
    "                )''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6870eb76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agg_trans\n",
      "agg_user\n",
      "map_trans\n",
      "map_user\n",
      "top_trans_dist\n",
      "top_trans_pin\n",
      "top_user_dist\n",
      "top_user_pin\n"
     ]
    }
   ],
   "source": [
    "# Now we need to verify if the tables have been uploaded to the database\n",
    "\n",
    "# Execute query to show tables in the database\n",
    "cursor.execute(\"SHOW TABLES;\")\n",
    "# Fetch all the data from the query  \n",
    "databases = cursor.fetchall()\n",
    "# loop through the data\n",
    "for db in databases:\n",
    "    # Print the database\n",
    "    print(db[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0a5a49",
   "metadata": {},
   "source": [
    "Now we have created the database, created the Tables next thing we need to do is push the data on the dataframe to respective tables on our database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cc7e3e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need to get our DFS as a dictionary and also put our Table columns also in a dictionary\n",
    "dfs = {\n",
    "    'agg_trans':agg_trans_df,\n",
    "    'agg_user':agg_user_df,\n",
    "    'map_trans':map_trans_df,\n",
    "    'map_user':map_user_df,\n",
    "    'top_trans_dist':top_trans_dist_df,\n",
    "    'top_trans_pin':top_trans_pin_df,\n",
    "    'top_user_dist':top_user_dist_df,\n",
    "    'top_user_pin':top_user_pin_df\n",
    "}\n",
    "\n",
    "table_columns = {\n",
    "    'agg_trans': list(agg_trans_df.columns),\n",
    "    'agg_user': list(agg_user_df.columns),\n",
    "    'map_trans': list(map_trans_df.columns),\n",
    "    'map_user': list(map_user_df.columns),\n",
    "    'top_trans_dist': list(top_trans_dist_df.columns),\n",
    "    'top_trans_pin': list(top_trans_pin_df.columns),\n",
    "    'top_user_dist': list(top_user_dist_df.columns),\n",
    "    'top_user_pin': list(top_user_pin_df.columns)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f13d477e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will write a function to push the data to the database\n",
    "\n",
    "# Create a function that accepts the connection, cursor, dataframe dictonary, and table columns dictionary\n",
    "def push_data_into_mysql(conn, cursor, dfs, table_columns):\n",
    "    # We loop through the keys in the df dictionary, hereby looping through the list names\n",
    "    for table_name in dfs.keys():\n",
    "        # Now we extract the dataframe\n",
    "        df = dfs[table_name]\n",
    "        # We extract the columns\n",
    "        columns = table_columns[table_name]\n",
    "        # Now we create placeholders for Insert into query\n",
    "        placeholders = ', '.join(['%s'] * len(columns))\n",
    "        # Create a body or emplate query where the dat will be inserted\n",
    "        query = f\"INSERT INTO {table_name} ({', '.join(columns)}) VALUES ({placeholders})\"\n",
    "        # Now we loop over the rows of the dataframe and extract just the data\n",
    "        for _, row in df.iterrows():\n",
    "            # Now we also loop through the columns then insert them in the row to extract the exact data we need, then save all in a tuple to avoid mixing it up, since they cant modified\n",
    "            data = tuple(row[column] for column in columns)\n",
    "            # Now we execute the query, also adding the respective data\n",
    "            cursor.execute(query, data)\n",
    "        # Next we commit to the connection\n",
    "        conn.commit()\n",
    "    # Print Success message, to show the function ran sucessful\n",
    "    print(\"Data successfully pushed into MySQL tables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bd7ad3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully pushed into MySQL tables\n"
     ]
    }
   ],
   "source": [
    "# Call the function with all it's dependencies \n",
    "push_data_into_mysql(conn, cursor, dfs, table_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3d6e9f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agg_trans table has 3594 rows and 7 columns and it's shape matches with the DataFrame.\n",
      "agg_user table has 6732 rows and 8 columns and it's shape matches with the DataFrame.\n",
      "map_trans table has 14636 rows and 9 columns and it's shape matches with the DataFrame.\n",
      "map_user table has 14640 rows and 9 columns and it's shape matches with the DataFrame.\n",
      "top_trans_dist table has 5920 rows and 9 columns and it's shape matches with the DataFrame.\n",
      "top_trans_pin table has 7137 rows and 7 columns and it's shape matches with the DataFrame.\n",
      "top_user_dist table has 5920 rows and 8 columns and it's shape matches with the DataFrame.\n",
      "top_user_pin table has 7140 rows and 6 columns and it's shape matches with the DataFrame.\n"
     ]
    }
   ],
   "source": [
    "# Now we need to verify if all data was uploaded sucessfully by cross checking with the dataframes and the table\n",
    "\n",
    "# Execute the query to show table in the current Database\n",
    "cursor.execute(\"SHOW TABLES\")\n",
    "# Fetch all tables\n",
    "tables = cursor.fetchall()\n",
    "\n",
    "# Loop through the Tables\n",
    "for table in tables:\n",
    "    # Extract the table\n",
    "    table_name = table[0]\n",
    "    # Execute the query to count the number of rows of the table\n",
    "    cursor.execute(f\"SELECT COUNT(*) FROM {table_name}\")\n",
    "    # Store the result in a variable\n",
    "    row_count = cursor.fetchone()[0]\n",
    "    # execute the query to count the columns in the table\n",
    "    cursor.execute(f\"SELECT COUNT(*) FROM information_schema.columns WHERE table_name='{table_name}'\")\n",
    "    # store the count in a variable \n",
    "    column_count = cursor.fetchone()[0]\n",
    "\n",
    "\n",
    "    # From the Dataframe select the exact dataframe using the table\n",
    "    df = dfs[table_name]\n",
    "    # Now we check if the Dataframe shape is equal to the Table row and column count\n",
    "    if df.shape == (row_count, column_count):\n",
    "        # If True print this\n",
    "        print(f\"{table_name} table has {row_count} rows and {column_count} columns and it's shape matches with the DataFrame.\")\n",
    "    else:\n",
    "        # If False print this\n",
    "        print(f\"{table_name} table has {row_count} rows and {column_count} columns but it's shape does not match with the DataFrame.\")\n",
    "\n",
    "# After verification, close connection\n",
    "# Close cursor\n",
    "cursor.close()\n",
    "# Then close the entire connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e904e4a",
   "metadata": {},
   "source": [
    "Now our data has been uploaded to our dataframes has been uploaded to our Database and the all have the same shape witht the Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33785cb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
